{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUFNk0BJAMZH"
   },
   "source": [
    "# Assignment 5: Extended Long Short-Term Memory (xLSTM)\n",
    "\n",
    "*Author:* Philipp Seidl\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment, we will explore the xLSTM architecture, a novel extension of the classic LSTM model. The paper can be found here: https://arxiv.org/abs/2405.04517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBfgx3oEAc3W"
   },
   "source": [
    "#### Background\n",
    "Recurrent Neural Networks (RNNs), particularly LSTMs, have proven highly effective in various sequence modeling tasks. However, the emergence of Transformers, with their parallel processing capabilities, has shifted the focus away from LSTMs, especially in large-scale language modeling.\n",
    "The xLSTM architecture aims to bridge this gap by enhancing LSTMs with mechanisms inspired by modern LLMs (e.g. block-structure, residual connections, ...).  Further, it introduces:\n",
    "- Exponential gating with normalization and stabilization techniques, which improves gradient flow and memory capacity.\n",
    "- Modifications to the LSTM memory structure, resulting in two variants:\n",
    "    - sLSTM: Employs a scalar memory with a scalar update rule and a new memory mixing technique through recurrent connections.\n",
    "    - mLSTM: Features a matrix memory, employs a covariance update rule, and is fully parallelizable, making it suitable for scaling.\n",
    "\n",
    "By integrating these extensions into residual block backbones, xLSTM blocks are formed, which can then be mixed and stacked to create the xLSTM architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08ut_E9kAdpU"
   },
   "source": [
    "## Exercise 1: Environment Setup\n",
    "\n",
    "When working with new architectures or specialized frameworks, it's essential to correctly set up the environment to ensure reproducibility. This exercise focuses on setting up the environment for working with the `xlstm` repository.\n",
    "\n",
    "1. Visit and clone the official repository: [https://github.com/NX-AI/xlstm](https://github.com/NX-AI/xlstm).  \n",
    "2. Set up the environment  \n",
    "3. Document your setup:  \n",
    "   - OS, Python version, Environment setup, CUDA version (if applicable), and GPU details.  \n",
    "   - Note any challenges you faced and how you resolved them. \n",
    "4. Submit your setup as a bash script using the IPython `%%bash` magic. Ensure it is reproducible.\n",
    "\n",
    "Getting only mLSTM working is sufficient (if you encounter issues with sLSTM cuda kernels)\n",
    "\n",
    "> **Note**: Depending on your system setup, you may need to adjust the `environment_pt220cu121.yaml` file, such as for the CUDA version. For this assignment, it is recommended to run it on GPUs. If you don't have one, consider using  [Colab](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true) or other online resources.\n",
    "\n",
    "> **Recommendations**: While the repository suggests using `conda`, we recommend using `mamba` or `micromamba` instead (way faster) (except if you are using colab). Learn more about them here: [https://mamba.readthedocs.io/en/latest/index.html](https://mamba.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "Questions to prepare: What is ninja, pytorch and cuda compatability and why do we care?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Bdb5fIMaKea1",
    "outputId": "fbea5037-812c-41a1-bd42-79f4b9790cd5"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "########## SOLUTION BEGIN ##########\n",
    "\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "id": "4FejZLBoK_Lo",
    "outputId": "37eadcd9-3134-45df-da5c-0db1a267f332"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify your installation of xLSTM:\n",
    "from omegaconf import OmegaConf\n",
    "from dacite import from_dict\n",
    "from dacite import Config as DaciteConfig\n",
    "from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig\n",
    "import os\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "use_slstm_kernels = False # set to True if you want to check if sLSTM cuda kernels are working\n",
    "\n",
    "xlstm_cfg = f\"\"\"\n",
    "mlstm_block:\n",
    "  mlstm:\n",
    "    conv1d_kernel_size: 4\n",
    "    qkv_proj_blocksize: 4\n",
    "    num_heads: 4\n",
    "slstm_block:\n",
    "  slstm:\n",
    "    backend: {'cuda' if use_slstm_kernels else 'vanilla'}\n",
    "    num_heads: 4\n",
    "    conv1d_kernel_size: 4\n",
    "    bias_init: powerlaw_blockdependent\n",
    "  feedforward:\n",
    "    proj_factor: 1.3\n",
    "    act_fn: gelu\n",
    "context_length: 32\n",
    "num_blocks: 7\n",
    "embedding_dim: 64\n",
    "slstm_at: [1,3,5] # empty = mLSTM only\n",
    "\"\"\"\n",
    "cfg = OmegaConf.create(xlstm_cfg)\n",
    "cfg = from_dict(data_class=xLSTMBlockStackConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n",
    "xlstm_stack = xLSTMBlockStack(cfg)\n",
    "\n",
    "x = torch.randn(4, 32, 64).to(DEVICE)\n",
    "xlstm_stack = xlstm_stack.to(DEVICE)\n",
    "y = xlstm_stack(x)\n",
    "y.shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbkUQdktAkeG"
   },
   "source": [
    "## Exercise 2: Understanding xLSTM Hyperparameters\n",
    "Explain key hyperparameters that influence the performance and behavior of the xLSTM architecture and explain how they influence total parameter count.\n",
    "The explanation should include: proj_factor, num_heads, act_fn, context_length, num_blocks, embedding_dim, hidden_size, dropout, slstm_at, qkv_proj_blocksize, conv1d_kernel_size. Also include how the matrix memory size of mLSTM is determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SOLUTION BEGIN ##########\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4rSyOdnAv6r"
   },
   "source": [
    "## Exercise 3: Train an xLSTM model on the Trump Dataset from the previous exercise\n",
    "Your task is to train an xLSTM model on the Trump Dataset from the previous exercise. \n",
    "- The goal is to achieve an average validation loss $\\mathcal{L}_{\\text{val}} < 1.35$. \n",
    "- You do not need to perform an extensive hyperparameter search, but you should document your runs. Log your runs with used hyperparameters using tools like wandb, neptune, mlflow, ... or a similar setup. Log training/validation loss and learning rate over steps as well as total trainable parameters of the model for each run.\n",
    "- You can use the training setup from the previous exercises or any setup of your choice using high level training libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "id": "WcX8FAV8DjtG",
    "outputId": "cbd2f947-f8cc-4cbf-b24f-a6eb14091ae5"
   },
   "outputs": [],
   "source": [
    "########## SOLUTION BEGIN ##########\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVADqjO1A9kI"
   },
   "source": [
    "## Exercise 4: Utilizing a Pretrained Model\n",
    "\n",
    "Foundation Models, those pretrained on large amounts of data are more and more important. We can use those models and fine-tune them on our dataset, rather than training them from scratch.\n",
    "Here are the things to consider:\n",
    "\n",
    "- Model Selection: Choose a pretrained language model from an online repository. Hint: You can explore platforms like Hugging Face (huggingface.co), which host numerous pretrained models.\n",
    "\n",
    "- Dataset: Use the Trump dataset with the same training and validation split as in previous exercises. You do not need to use character tokenization.\n",
    "\n",
    "- Performance Evaluation: Evaluate the performance of the pretrained model on the validation set before and during fine-tuning. Report average-CE-loss as well as an example generated sequence with the same prompt for each epoch.\n",
    " \n",
    "- Fine-tuning: Adjust the learning rate, potentially freeze some layers, train for a few epochs with a framework of your choice (e.g. [lightning](https://lightning.ai/docs/pytorch/stable/), [huggingface](https://huggingface.co/models), ...)\n",
    "\n",
    "- Computational Resources: Be mindful of the computational demands of pretrained models. You might need access to GPUs. Try to keep the model size at a minimum and go for e.g. distilled versions or other small LMs\n",
    "\n",
    "- Hyperparameter Tuning: You can experiment with different learning rates and potentially other hyperparameters during fine-tuning but no need to do this in depth\n",
    "\n",
    "By completing this exercise, you will gain experience with utilizing pretrained models, understanding their capabilities, and the process of fine-tuning. Decreasing the validation loss can be seen as a success for this exercise.\n",
    "\n",
    "> **Note**: This is a standalone exercise and doesn't build upon the previous tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqv4tH69Ab0X"
   },
   "outputs": [],
   "source": [
    "########## SOLUTION BEGIN ##########\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: The Memory-Matrix Capacity - Associative Recall\n",
    "\n",
    "One of the central claims of the xLSTM paper is that the **mLSTM** (matrix LSTM) possesses a superior memory capacity compared to the **sLSTM** (scalar LSTM) due to its matrix cell state $C_t$. While sLSTM manages a scalar memory similar to traditional LSTMs, mLSTM utilizes a matrix memory updated via a covariance rule, effectively functioning as a Key-Value store.\n",
    "\n",
    "**Task:**\n",
    "Demonstrate this difference empirically using a synthetic \"Associative Recall\" task.\n",
    "\n",
    "We have provided a data generator function `generate_associative_data` below. This function creates sequences of Key-Value pairs followed by a Query Key.\n",
    "* **Format:** `k1, v1, k2, v2, ..., k_query`\n",
    "* **Goal:** The model must predict the value associated with `k_query`.\n",
    "* **Example:** For the sequence `7, 3, 4, 2, ..., 7`, the model should return `3` (recalling that 7 was paired with 3).\n",
    "\n",
    "**Your Goal:**\n",
    "1.  **Instantiate models** with comparable parameter counts:\n",
    "    * **Model A (sLSTM):** A stack consisting only of sLSTM blocks.\n",
    "    * **Model B (mLSTM):** A stack consisting only of mLSTM blocks.\n",
    "    * **Model C (LSTM):** (Optional) A standard PyTorch LSTM.\n",
    "    * **Model D (Transformer):** (Optional) A standard Transformer (e.g., GPT-2 style).\n",
    "    * *Note:* Keep dimensions small (e.g., `embedding_dim=16`, `num_blocks=2`) for fast iteration.\n",
    "2.  **Train the models** on the generated data for 1000 steps.\n",
    "3.  **Vary the head_dim**: try different head dimensions (e.g., 4, 16, 32, 64) while keeping the embedding dimension fixed.\n",
    "    * *Hypothesis:* How does the matrix memory capacity change as `head_dim` increases?\n",
    "4.  **Plot your Results**: Visualize the validation accuracy over time for the different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "from dacite import from_dict, Config as DaciteConfig\n",
    "from xlstm import xLSTMBlockStack, xLSTMBlockStackConfig\n",
    "\n",
    "def generate_associative_data(batch_size, num_pairs, vocab_size, device):\n",
    "    \"\"\"\n",
    "    Generates batch: [k1, v1, k2, v2, ..., k_query]\n",
    "    Target: v_query\n",
    "    \"\"\"\n",
    "    # Create random pairs\n",
    "    keys = torch.randint(0, vocab_size, (batch_size, num_pairs))\n",
    "    vals = torch.randint(0, vocab_size, (batch_size, num_pairs))\n",
    "    \n",
    "    # Select a query index for each batch item\n",
    "    query_indices = torch.randint(0, num_pairs, (batch_size,))\n",
    "    \n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        # Interleave keys and values\n",
    "        seq = torch.stack((keys[b], vals[b]), dim=1).flatten() \n",
    "        query_key = keys[b, query_indices[b]]\n",
    "        target_val = vals[b, query_indices[b]]\n",
    "        \n",
    "        # Sequence: k1, v1, k2, v2, ..., query_key\n",
    "        inputs.append(torch.cat([seq, query_key.unsqueeze(0)]))\n",
    "        targets.append(target_val)\n",
    "        \n",
    "    return torch.stack(inputs).to(device), torch.stack(targets).to(device)\n",
    "\n",
    "# 1. Configuration - adjust to your liking\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "VOCAB_SIZE = 64\n",
    "NUM_PAIRS = 16 \n",
    "EMBED_DIM = 16 \n",
    "BATCH_SIZE = 512\n",
    "STEPS = 1000\n",
    "\n",
    "########## SOLUTION BEGIN ##########\n",
    "\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Visualizing Learned Positional Encodings\n",
    "\n",
    "In Exercise 5, we saw that the **Transformer** required explicit `pos_embedding` parameters to function, whereas the **xLSTM** models did not. This is because Transformers process data in parallel (permutation invariant), while xLSTMs process data sequentially (time is implicit).\n",
    "\n",
    "Since we used **learnable** positional embeddings for the Transformer, the model had to *discover* how to represent \"position\" from scratch during training.\n",
    "\n",
    "**Task:**\n",
    "1.  Extract the learned positional embedding weights from your trained model (from Exercise 5, the pretrained one from Exercise 4, or the one from Assignment 4).\n",
    "2.  Compute the **Cosine Similarity Matrix** between all positions.\n",
    "    * *Goal:* We want a matrix $M$ of size $(T \\times T)$ where $M_{i,j}$ represents how similar the embedding at position $i$ is to position $j$.\n",
    "3.  Visualize this matrix - What do we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SOLUTION BEGIN ##########\n",
    "\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "xlstm (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
